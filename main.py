#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
    Predix.io Catalog Scraper
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Scrape the Predix.io Catalog and generate an excel file listing
    all the services with detailed information available on it.

    :version: 0.1
    :copyright: 2016 by Mirco Veltri
    :license: GPL, see LICENSE for more details
"""

import os
import sys
import shutil
import subprocess
import signal
import configs  # configuration settings come from configs.py
from utils.webpage_spider import WebPageSpider
from utils.predix_catalog_scraper import PredixCatalogScraper
from utils.excel_file_writer import ExcelFileWriter

OUTPUT_FOLDER = "output"
OUTPUT_FILENAME = "predix-catalog.xlsx"


def services():
    return "services"


def analytics():
    return "analytics"

catalogs = {
    0: services,
    1: analytics
}


def cleanup():
    """ Removing pyc files and clean """
    ghostdriver_log_file = "ghostdriver.log"
    if os.path.exists(ghostdriver_log_file):
        os.remove(ghostdriver_log_file)

    if sys.platform == "linux" or sys.platform == "linux2" or sys.platform == "darwin":
        subprocess.check_output(
            ['find', '.', '-name', '*.pyc', '-exec', 'rm', '-rf', '{}', ';'])
    elif sys.platform == "win32":
        subprocess.check_output(['del', '/S', '*.pyc'])
    else:
        exit(0)


def sigint_handler(signum, frame):
    """ Handle CTRL+C in the script """
    cleanup()
signal.signal(signal.SIGINT, sigint_handler)


def main():
    """ The main program """
    # Create a webpage spider instance
    web_spider = WebPageSpider()
    # Scraping the Services Catalog
    print "\n# Scraping Predix Catalog for 'Services': " + configs.px_services_catalog_url
    services_scraper = PredixCatalogScraper(web_spider, catalogs[0](), configs)
    services_scraper.parse()

    # Scraping the Analytics Catalog
    print "\n# Scraping Predix Catalog for 'Analytics': " + configs.px_analytics_catalog_url
    analytics_scraper = PredixCatalogScraper(
        web_spider, catalogs[1](), configs)
    analytics_scraper.parse()

    # Create an excel workbook
    file_writer = ExcelFileWriter()
    file_writer.create_file(OUTPUT_FOLDER, OUTPUT_FILENAME)

    # Save and close the Excel file
    print "    * Saving the file..."
    services_scraper.write_to_file(file_writer)
    analytics_scraper.write_to_file(file_writer)
    file_writer.close()
    print "    * '" + os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME) + "' file saved"

    # Removing autogenerated files
    cleanup()

    # Output to shell
    print "\n\n## Summary for 'Services':"
    print "    * number of available categories:" + str(services_scraper.categoriesCounter())
    print "    * number of available services:" + str(services_scraper.tilesCounter())

    print "\n## Summary for 'Analytics':"
    print "    * number of available categories:" + str(analytics_scraper.categoriesCounter())
    print "    * number of available services:" + str(analytics_scraper.tilesCounter())

if __name__ == "__main__":
    # Run as main program
    main()
